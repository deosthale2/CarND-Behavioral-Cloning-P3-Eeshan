{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vrtcuser/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/vrtcuser/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/vrtcuser/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/vrtcuser/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/vrtcuser/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/vrtcuser/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout, Lambda\n",
    "from keras.layers import Cropping2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(images, measurements, to_flip = 0):\n",
    "    X_train, y_train = images, measurements\n",
    "    if to_flip == 1:\n",
    "        flip_measurements = -1.0*measurements\n",
    "        flip_images = []\n",
    "        for image in images:\n",
    "            flip_images += [cv2.flip(image, 1)]\n",
    "        X_train = np.concatenate((X_train, flip_images), axis = 0)\n",
    "        y_train = np.concatenate((y_train, flip_measurements), axis = 0)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(samples, batch_size = 32, is_validation = 0, include_side = 0, to_flip = 0):\n",
    "    samples = random.sample(samples, k=2000) if is_validation == 0 else shuffle(samples)\n",
    "    num_samples = len(samples)\n",
    "    while  True:\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            center_images, left_images, right_images = [], [], []\n",
    "            center_measurements = []\n",
    "            for batch_sample in batch_samples:\n",
    "                center_images += [cv2.imread('./data/'+batch_sample[0])]\n",
    "                center_measurements += [float(batch_sample[3])]\n",
    "                if include_side == 1:\n",
    "                    left_images += [cv2.imread('./data/'+batch_sample[1].split(' ')[-1])]\n",
    "                    right_images += [cv2.imread('./data/'+batch_sample[2].split(' ')[-1])]\n",
    "            images = np.array(center_images)\n",
    "            measurements = np.array(center_measurements)\n",
    "            if include_side == 1:\n",
    "                images = np.concatenate((images, left_images, right_images), axis = 0)\n",
    "                measurements = np.concatenate((measurements, measurements + 0.2, measurements - 0.2), axis = 0)\n",
    "            if is_validation == 0:\n",
    "                X_train, y_train = preprocess_image(images, measurements, to_flip = to_flip)\n",
    "            else:\n",
    "                X_train, y_train = images, measurements\n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_nvidia(train_samples, validation_samples):\n",
    "    batch_size = 32\n",
    "    train_generator = generator(train_samples, batch_size = batch_size, is_validation = 0, include_side = 1, to_flip = 0)\n",
    "    validation_generator = generator(validation_samples, batch_size = batch_size, is_validation = 1, include_side = 0)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: (x/255.0) - 0.5, input_shape=(160,320,3)))\n",
    "    model.add(Cropping2D(cropping=((70,25),(0,0))))\n",
    "    model.add(Conv2D(24, 5, strides = (2,2), activation='relu'))\n",
    "    model.add(Conv2D(36, 5, strides = (2,2), activation='relu'))\n",
    "    model.add(Conv2D(48, 5, strides = (2,2), activation='relu'))\n",
    "    model.add(Conv2D(64, 3, activation='relu'))\n",
    "    model.add(Conv2D(64, 3, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100))\n",
    "    model.add(Dense(50))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit_generator(train_generator, \n",
    "                        steps_per_epoch = 2000//batch_size, \n",
    "                        validation_data = validation_generator, \n",
    "                        validation_steps = len(validation_samples)//batch_size, \n",
    "                        epochs = 8, verbose = 1)\n",
    "    model.save('model_nvidia.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uniform(samples):\n",
    "    no_bins = 25\n",
    "    augmented_samples = []\n",
    "    count_thresh = int(len(samples)/no_bins)*1\n",
    "    samples_arr = np.array(samples)\n",
    "    angles = np.array(list(map(float, samples_arr[:,3])))\n",
    "    angle_bins = np.linspace(-1., 1.01, no_bins + 1)\n",
    "    fig, ax = plt.subplots(2)\n",
    "    ax[0].hist(angles, angle_bins, rwidth = 0.9)\n",
    "    print(len(angles))\n",
    "    for i in range(no_bins):\n",
    "        idx = np.where((angles>=angle_bins[i]) & (angles<angle_bins[i+1]))[0]\n",
    "        if len(idx) < count_thresh and len(idx) > 0:\n",
    "            idx_sel = np.random.choice(idx, count_thresh - len(idx))\n",
    "            samples = samples + samples_arr[idx_sel].tolist()\n",
    "    samples_arr = np.array(samples)\n",
    "    angles = np.array(list(map(float, samples_arr[:,3])))\n",
    "    print(len(angles))\n",
    "    ax[1].hist(angles, angle_bins, rwidth = 0.9)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    samples = []\n",
    "    with open('./data/driving_log.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            if line[0] == 'center':\n",
    "                continue\n",
    "            samples += [line]\n",
    "    samples = make_uniform(samples)\n",
    "\n",
    "    train_samples, validation_samples = train_test_split(samples, test_size = 0.2)\n",
    "\n",
    "    model_nvidia(train_samples, validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8036\n",
      "12648\n",
      "Epoch 1/15\n",
      "62/62 [==============================] - 119s 2s/step - loss: 0.0778 - acc: 0.1233 - val_loss: 0.0539 - val_acc: 0.3592\n",
      "Epoch 2/15\n",
      "62/62 [==============================] - 110s 2s/step - loss: 0.0404 - acc: 0.1267 - val_loss: 0.0311 - val_acc: 0.3639\n",
      "Epoch 3/15\n",
      "62/62 [==============================] - 108s 2s/step - loss: 0.0259 - acc: 0.1262 - val_loss: 0.0260 - val_acc: 0.3631\n",
      "Epoch 4/15\n",
      "62/62 [==============================] - 98s 2s/step - loss: 0.0185 - acc: 0.1269 - val_loss: 0.0195 - val_acc: 0.3635\n",
      "Epoch 5/15\n",
      "62/62 [==============================] - 102s 2s/step - loss: 0.0146 - acc: 0.1269 - val_loss: 0.0206 - val_acc: 0.3643\n",
      "Epoch 6/15\n",
      "62/62 [==============================] - 94s 2s/step - loss: 0.0119 - acc: 0.1277 - val_loss: 0.0190 - val_acc: 0.3627\n",
      "Epoch 7/15\n",
      "62/62 [==============================] - 100s 2s/step - loss: 0.0096 - acc: 0.1272 - val_loss: 0.0188 - val_acc: 0.3655\n",
      "Epoch 8/15\n",
      "62/62 [==============================] - 94s 2s/step - loss: 0.0091 - acc: 0.1274 - val_loss: 0.0264 - val_acc: 0.3615\n",
      "Epoch 9/15\n",
      "62/62 [==============================] - 97s 2s/step - loss: 0.0081 - acc: 0.1275 - val_loss: 0.0211 - val_acc: 0.3639\n",
      "Epoch 10/15\n",
      " 4/62 [>.............................] - ETA: 1:13 - loss: 0.0078 - acc: 0.1198"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-715fdbe3d50e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel_nvidia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-cec1405c11c9>\u001b[0m in \u001b[0;36mmodel_nvidia\u001b[0;34m(train_samples, validation_samples)\u001b[0m\n\u001b[1;32m     23\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                         epochs = 15, verbose = 1)\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_nvidia.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF31JREFUeJzt3X2MXGXdxvHvZSslhggtbRBawpY8VawxAbKpRBKRlwcKGLbGgmsEK5ZUsRiMGi3yBwYlFv8QJb4gAQTUWLBIWAVCCm1jTCiwyJttU7q8GFoLrbRUDbFa+D1/nHv7HMtMZ2b3zNld7uuTbPac+9znzG/umZ1rzsvMKiIwM7P8vGOsCzAzs7HhADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy5QDwMwsUw4AM7NMOQDMzDI1eawLOJDp06dHT0/PWJdhZjahPP7443+LiBmt+o3rAOjp6WFwcHCsyzAzm1Ak/aWdfj4EZGaWKQeAmVmmHABmZpka1+cAzMaDnmX3tt33xeXndrESs2p5D8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8tU2wEgaZKkJyT9Ps3PlvSIpCFJd0g6KLVPSfNDaXlPaRtXpPZNks6q+s6YmVn7OtkDuBzYWJq/FrguIv4H2AUsTu2LgV2p/brUD0lzgX7gA8B84CeSJo2ufDMzG6m2AkDSLOBc4KY0L+A0YGXqchuwIE33pXnS8tNT/z5gRUTsiYgXgCFgXhV3wszMOtfuHsAPgK8Db6b5w4HXImJvmt8CzEzTM4GXANLy3an/vvYG6+wjaYmkQUmDO3bs6OCumJlZJ1oGgKSPAdsj4vEa6iEiboyI3ojonTGj5T+1NzOzEWrnP4KdDJwn6RzgYODdwA+BwyRNTu/yZwFbU/+twNHAFkmTgUOBV0vtw8rrmJlZzVruAUTEFRExKyJ6KE7iro6ITwNrgIWp2yLgnjQ9kOZJy1dHRKT2/nSV0GxgDvBoZffEzMw6Mpr/CfwNYIWk7wBPADen9puBX0gaAnZShAYRsV7SncAGYC+wNCLeGMXtm5nZKHQUABGxFlibpp+nwVU8EfEv4Pwm618DXNNpkWZmVj1/EtjMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTLUMAElHS1ojaYOk9ZIuT+3TJK2StDn9npraJel6SUOSnpZ0Ymlbi1L/zZIWde9umZlZK+3sAewFvhoRc4GTgKWS5gLLgIciYg7wUJoHOBuYk36WAD+FIjCAq4APAfOAq4ZDw8zM6tcyACJiW0T8KU3/A9gIzAT6gNtSt9uABWm6D7g9CuuAwyQdCZwFrIqInRGxC1gFzK/03piZWds6OgcgqQc4AXgEOCIitqVFLwNHpOmZwEul1baktmbt+9/GEkmDkgZ37NjRSXlmZtaBtgNA0iHAXcCXI+Lv5WUREUBUUVBE3BgRvRHRO2PGjCo2aWZmDbQVAJLeSfHi/6uI+G1qfiUd2iH93p7atwJHl1afldqatZuZ2Rho5yogATcDGyPi+6VFA8DwlTyLgHtK7Z9JVwOdBOxOh4oeAM6UNDWd/D0ztZmZ2RiY3Eafk4GLgGckPZnavgksB+6UtBj4C3BBWnYfcA4wBLwOXAwQETslfRt4LPW7OiJ2VnIvzMysYy0DICL+CKjJ4tMb9A9gaZNt3QLc0kmBZmbWHf4ksJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlikHgJlZphwAZmaZcgCYmWXKAWBmlqmW/xTe7O2mZ9m9bfd9cfm5Xazk/43Hmuztz3sAZmaZ8h6AWcX8bt4mCu8BmJllygFgZpYpHwKyCc+HXMxGxnsAZmaZcgCYmWXKAWBmlikHgJlZpmo/CSxpPvBDYBJwU0Qsr7sGq89ITtD6pG71PKbWSK0BIGkS8GPgf4EtwGOSBiJiQ511mE10fkG3KtS9BzAPGIqI5wEkrQD6AAfAGOn0hcQvPHnw45wHRUR9NyYtBOZHxCVp/iLgQxFxWanPEmBJmn0fsGmENzcd+Nsoyu0W19UZ19UZ19WZt2tdx0TEjFadxt0HwSLiRuDG0W5H0mBE9FZQUqVcV2dcV2dcV2dyr6vuq4C2AkeX5melNjMzq1ndAfAYMEfSbEkHAf3AQM01mJkZNR8Cioi9ki4DHqC4DPSWiFjfpZsb9WGkLnFdnXFdnXFdncm6rlpPApuZ2fjhTwKbmWXKAWBmlqkJHQCSzpe0XtKbkppeMiVpvqRNkoYkLSu1z5b0SGq/I52YrqKuaZJWSdqcfk9t0OdUSU+Wfv4laUFadqukF0rLjq+rrtTvjdJtD5Tax3K8jpf0cHq8n5b0ydKySser2fOltHxKuv9DaTx6SsuuSO2bJJ01mjo6rOkrkjaksXlI0jGlZQ0fzxpr+6ykHaUaLiktW5Qe982SFtVY03Wlep6V9FppWdfGS9ItkrZL+nOT5ZJ0far7aUknlpZVP1YRMWF/gPdTfFhsLdDbpM8k4DngWOAg4Clgblp2J9Cfpm8ALq2oru8By9L0MuDaFv2nATuBd6X5W4GFXRivtuoC/tmkfczGC3gvMCdNHwVsAw6rerwO9Hwp9fkicEOa7gfuSNNzU/8pwOy0nUk11XRq6flz6XBNB3o8axyvzwI/arDuNOD59Htqmp5aR0379f8SxQUpdYzXR4ATgT83WX4OcD8g4CTgkW6O1YTeA4iIjRHR6pPC+75+IiL+DawA+iQJOA1YmfrdBiyoqLS+tL12t7sQuD8iXq/o9pvptK59xnq8IuLZiNicpv8KbAdaftJxBBo+Xw5Q70rg9DQ+fcCKiNgTES8AQ2l7Xa8pItaUnj/rKD5jU4d2xquZs4BVEbEzInYBq4D5Y1DTp4BfV3C7LUXEHyje7DXTB9wehXXAYZKOpEtjNaEDoE0zgZdK81tS2+HAaxGxd7/2KhwREdvS9MvAES369/PWJ+A1aRfwOklTaq7rYEmDktYNH5ZiHI2XpHkU7+yeKzVXNV7Nni8N+6Tx2E0xPu2s262ayhZTvIsc1ujxrEq7tX0iPT4rJQ1/GHTMxysdKpsNrC41d3O8WmlWe1fGatx9FcT+JD0IvKfBoisj4p666xl2oLrKMxERkppea5vS/YMUn40YdgXFC+FBFNcDfwO4usa6jomIrZKOBVZLeobiRW7EKh6vXwCLIuLN1Dzi8Xq7kXQh0AucUmp+y+MZEc813kJX/A74dUTskfR5ir2n02q8/QPpB1ZGxBultrEer9qM+wCIiDNGuYlmXz/xKsXu1eT0Lq6jr6U4UF2SXpF0ZERsSy9Y2w+wqQuAuyPiP6VtD78b3iPp58DX6qwrIram389LWgucANzFGI+XpHcD91KE/7rStkc8Xg2083Ulw322SJoMHErxfOrWV520tV1JZ1AE6ikRsWe4vcnjWdULWsvaIuLV0uxNFOd8htf96H7rrq2jppJ+YGm5ocvj1Uqz2rsyVjkcAmr49RNRnFlZQ3H8HWARUNUexUDaXjvbfcvxx/QiOHzcfQHQ8IqBbtQlaerwIRRJ04GTgQ1jPV7psbub4vjoyv2WVTle7XxdSbnehcDqND4DQL+Kq4RmA3OAR0dRS9s1SToB+BlwXkRsL7U3fDwrqKmT2o4szZ4HbEzTDwBnphqnAmfy33vCXasp1XUcxQnVh0tt3R6vVgaAz6SrgU4Cdqc3ON0ZqyrPcNf9A3yc4ljYHuAV4IHUfhRwX6nfOcCzFCl+Zan9WIo/0CHgN8CUiuo6HHgI2Aw8CExL7b0U/wVtuF8PRbK/Y7/1VwPPULyQ/RI4pK66gA+n234q/V48HsYLuBD4D/Bk6ef4boxXo+cLxSGl89L0wen+D6XxOLa07pVpvU3A2RU+11vV9GD6Gxgem4FWj2eNtX0XWJ9qWAMcV1r3c2kch4CL66opzX8LWL7fel0dL4o3e9vSc3kLxfmaLwBfSMtF8U+znku331tat/Kx8ldBmJllKodDQGZm1oADwMwsUw4AM7NMjevLQKdPnx49PT1jXYaZ2YTy+OOP/y0m4v8ELuvp6WFwcHCsyzAzm1Ak/aWdfj4EZGaWKQeAmVmmHABmZpka1+cAzMaDnmX3tt33xeXndrESs2p5D8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTDkAzMwy1XYASJok6QlJv0/zsyU9ImlI0h2SDkrtU9L8UFreU9rGFal9k6Szqr4zZmbWvk72AC4HNpbmrwWui4j/AXYBi1P7YmBXar8u9UPSXKAf+AAwH/iJpEmjK9/MzEaqrQCQNAs4F7gpzQs4DViZutwGLEjTfWmetPz01L8PWBEReyLiBWAImFfFnTAzs861uwfwA+DrwJtp/nDgtYjYm+a3ADPT9EzgJYC0fHfqv6+9wTpmZlazlgEg6WPA9oh4vIZ6kLRE0qCkwR07dtRxk2ZmWWpnD+Bk4DxJLwIrKA79/BA4TNLwP5SZBWxN01uBowHS8kOBV8vtDdbZJyJujIjeiOidMaPlP7U3M7MRahkAEXFFRMyKiB6Kk7irI+LTwBpgYeq2CLgnTQ+kedLy1RERqb0/XSU0G5gDPFrZPTEzs46M5l9CfgNYIek7wBPAzan9ZuAXkoaAnRShQUSsl3QnsAHYCyyNiDdGcftmZjYKHQVARKwF1qbp52lwFU9E/As4v8n61wDXdFqkmZlVz58ENjPLlAPAzCxTDgAzs0w5AMzMMuUAMDPLlAPAzCxTDgAzs0w5AMzMMuUAMDPLlAPAzCxTDgAzs0w5AMzMMuUAMDPLlAPAzCxTDgAzs0w5AMzMMuUAMDPLlAPAzCxTDgAzs0w5AMzMMuUAMDPLlAPAzCxTDgAzs0w5AMzMMuUAMDPLlAPAzCxTDgAzs0w5AMzMMuUAMDPLlAPAzCxTLQNA0tGS1kjaIGm9pMtT+zRJqyRtTr+npnZJul7SkKSnJZ1Y2tai1H+zpEXdu1tmZtZKO3sAe4GvRsRc4CRgqaS5wDLgoYiYAzyU5gHOBuaknyXAT6EIDOAq4EPAPOCq4dAwM7P6tQyAiNgWEX9K0/8ANgIzgT7gttTtNmBBmu4Dbo/COuAwSUcCZwGrImJnROwCVgHzK703ZmbWto7OAUjqAU4AHgGOiIhtadHLwBFpeibwUmm1LamtWbuZmY2BtgNA0iHAXcCXI+Lv5WUREUBUUZCkJZIGJQ3u2LGjik2amVkDbQWApHdSvPj/KiJ+m5pfSYd2SL+3p/atwNGl1Weltmbt/yUiboyI3ojonTFjRif3xczMOtDOVUACbgY2RsT3S4sGgOEreRYB95TaP5OuBjoJ2J0OFT0AnClpajr5e2ZqMzOzMTC5jT4nAxcBz0h6MrV9E1gO3ClpMfAX4IK07D7gHGAIeB24GCAidkr6NvBY6nd1ROys5F6YmVnHWgZARPwRUJPFpzfoH8DSJtu6BbilkwLNzKw7/ElgM7NMOQDMzDLlADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy5QDwMwsUy3/KbyZdV/Psnvb7vvi8nO7WInlxAFg2fGLrVnBh4DMzDLlPQCzinkPwyYK7wGYmWXKewA24fkdt9nIeA/AzCxTDgAzs0y9rQ8BdfvQQKfb73b/8VhTHfchR2+Hx2289a/DeKvJewBmZpmqPQAkzZe0SdKQpGV1376ZmRVqDQBJk4AfA2cDc4FPSZpbZw1mZlaoew9gHjAUEc9HxL+BFUBfzTWYmRn1B8BM4KXS/JbUZmZmNVNE1Hdj0kJgfkRckuYvAj4UEZeV+iwBlqTZ9wGbRnhz04G/jaLcbnFdnXFdnXFdnXm71nVMRMxo1anuy0C3AkeX5meltn0i4kbgxtHekKTBiOgd7Xaq5ro647o647o6k3tddR8CegyYI2m2pIOAfmCg5hrMzIya9wAiYq+ky4AHgEnALRGxvs4azMysUPsngSPiPuC+Gm5q1IeRusR1dcZ1dcZ1dSbrumo9CWxmZuOHvwrCzCxTEzoAJJ0vab2kNyU1PWPe7Osn0snoR1L7HenEdBV1TZO0StLm9Htqgz6nSnqy9PMvSQvSslslvVBadnxddaV+b5Rue6DUPpbjdbykh9Pj/bSkT5aWVTperb6uRNKUdP+H0nj0lJZdkdo3STprNHV0WNNXJG1IY/OQpGNKyxo+njXW9llJO0o1XFJatig97pslLaqxputK9Twr6bXSsq6Nl6RbJG2X9OcmyyXp+lT305JOLC2rfqwiYsL+AO+n+KzAWqC3SZ9JwHPAscBBwFPA3LTsTqA/Td8AXFpRXd8DlqXpZcC1LfpPA3YC70rztwILuzBebdUF/LNJ+5iNF/BeYE6aPgrYBhxW9Xgd6PlS6vNF4IY03Q/ckabnpv5TgNlpO5NqqunU0vPn0uGaDvR41jhenwV+1GDdacDz6ffUND21jpr26/8ligtS6hivjwAnAn9usvwc4H5AwEnAI90cqwm9BxARGyOi1QfFGn79hCQBpwErU7/bgAUVldaXttfudhcC90fE6xXdfjOd1rXPWI9XRDwbEZvT9F+B7UDLD7qMQDtfV1KudyVwehqfPmBFROyJiBeAobS9rtcUEWtKz591FJ+xqcNovt7lLGBVROyMiF3AKmD+GNT0KeDXFdxuSxHxB4o3e830AbdHYR1wmKQj6dJYTegAaFOzr584HHgtIvbu116FIyJiW5p+GTiiRf9+3voEvCbtAl4naUrNdR0saVDSuuHDUoyj8ZI0j+Kd3XOl5qrGq52vK9nXJ43Hborx6dZXnXS63cUU7yKHNXo8q9JubZ9Ij89KScMfBh3z8UqHymYDq0vN3RyvVprV3pWxGvf/EEbSg8B7Giy6MiLuqbueYQeqqzwTESGp6aVWKd0/SPHZiGFXULwQHkRxOdg3gKtrrOuYiNgq6VhgtaRnKF7kRqzi8foFsCgi3kzNIx6vtxtJFwK9wCml5rc8nhHxXOMtdMXvgF9HxB5Jn6fYezqtxts/kH5gZUS8UWob6/GqzbgPgIg4Y5SbaPb1E69S7F5NTu/i3vK1FCOtS9Irko6MiG3pBWv7ATZ1AXB3RPyntO3hd8N7JP0c+FqddUXE1vT7eUlrgROAuxjj8ZL0buBeivBfV9r2iMergZZfV1Lqs0XSZOBQiudTO+t2qyYknUERqKdExJ7h9iaPZ1UvaO18vcurpdmbKM75DK/70f3WXVtHTSX9wNJyQ5fHq5VmtXdlrHI4BNTw6yeiOLOyhuL4O8AioKo9ioG0vXa2+5bjj+lFcPi4+wKg4RUD3ahL0tThQyiSpgMnAxvGerzSY3c3xfHRlfstq3K82vm6knK9C4HVaXwGgH4VVwnNBuYAj46ilrZrknQC8DPgvIjYXmpv+HhWUFMntR1Zmj0P2JimHwDOTDVOBc7kv/eEu1ZTqus4ihOqD5fauj1erQwAn0lXA50E7E5vcLozVlWe4a77B/g4xbGwPcArwAOp/SjgvlK/c4BnKVL8ylL7sRR/oEPAb4ApFdV1OPAQsBl4EJiW2nuBm0r9eiiS/R37rb8aeIbiheyXwCF11QV8ON32U+n34vEwXsCFwH+AJ0s/x3djvBo9XygOKZ2Xpg9O938ojcexpXWvTOttAs6u8LneqqYH09/A8NgMtHo8a6ztu8D6VMMa4LjSup9L4zgEXFxXTWn+W8Dy/dbr6nhRvNnblp7LWyjO13wB+EJaLop/mvVcuv3e0rqVj5U/CWxmlqkcDgGZmVkDDgAzs0w5AMzMMuUAMDPLlAPAzCxTDgAzs0w5AMzMMuUAMDPL1P8BzOgL2QDVub8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
